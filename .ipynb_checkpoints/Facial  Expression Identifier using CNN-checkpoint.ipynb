{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, Dropout, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    train_data=[]\n",
    "    train_label=[]\n",
    "    test_data=[]\n",
    "    test_label=[]\n",
    "    with open(data_path,'r') as file:\n",
    "        for line_no,line in enumerate(file.readlines()):\n",
    "            if(0<line_no<=35887):\n",
    "                class_type,pixels,use_type=line.split(',')\n",
    "                \n",
    "                #In dataest images are in 48*48 dimesion.So we extract the image data and rearrange it to 48*48 array\n",
    "                image_data=np.asarray([int(x) for x in pixels.split()]).reshape(48,48)  \n",
    "                \n",
    "                #We convert the image data into unsigned integer and divide it by 255 to normalize the data.\n",
    "                #255 is the maximum possible value of a single cell. By dividing every element by 255, we ensure that all our values range between 0 and 1.\n",
    "                image_data=image_data.astype(np.uint8)/255.0\n",
    "              \n",
    "                if (use_type.strip() == 'PrivateTest'):    \n",
    "                    test_data.append(image_data)\n",
    "                    test_label.append(class_type)\n",
    "                else:\n",
    "                    train_data.append(image_data)\n",
    "                    train_label.append(class_type)\n",
    "        \n",
    "        #we will expand the dimensions of both testing and training data by one to accommodate the channel \n",
    "        test_data=np.expand_dims(test_data,-1)\n",
    "        train_data=np.expand_dims(train_data,-1)\n",
    "        \n",
    "        #one hot encode all the labels using the to_categorical() function \n",
    "        test_label=to_categorical(test_label, num_classes=7)\n",
    "        train_label=to_categorical(train_label, num_classes=7)\n",
    "        \n",
    "        return np.array(train_data),np.array(train_label),np.array(test_data),np.array(test_label)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in Training set: 32298\n",
      "Number of images in Test set: 3589\n"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "path=\"D:/Github Projects/CNN/Facial Expression/fer2013.csv\"\n",
    "train_data,train_label,test_data,test_label=load_data(path)\n",
    "print(\"Number of images in Training set:\", len(train_data))\n",
    "print(\"Number of images in Test set:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x000001691B043D30>>\n",
      "Train on 25838 samples, validate on 6460 samples\n",
      "Epoch 1/100\n",
      "25838/25838 [==============================] - 175s 7ms/step - loss: 2.1762 - acc: 0.1957 - val_loss: 1.8931 - val_acc: 0.2489\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.89312, saving model to D:/Github Projects/CNN/Facial Expression/weights.hd5\n",
      "Epoch 2/100\n",
      "25838/25838 [==============================] - 167s 6ms/step - loss: 1.8792 - acc: 0.2420 - val_loss: 1.8547 - val_acc: 0.2489\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.89312 to 1.85466, saving model to D:/Github Projects/CNN/Facial Expression/weights.hd5\n",
      "Epoch 3/100\n",
      "25838/25838 [==============================] - 163s 6ms/step - loss: 1.8453 - acc: 0.2495 - val_loss: 1.8313 - val_acc: 0.2489\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.85466 to 1.83127, saving model to D:/Github Projects/CNN/Facial Expression/weights.hd5\n",
      "Epoch 4/100\n",
      "25838/25838 [==============================] - 164s 6ms/step - loss: 1.8323 - acc: 0.2509 - val_loss: 1.8227 - val_acc: 0.2489\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.83127 to 1.82272, saving model to D:/Github Projects/CNN/Facial Expression/weights.hd5\n",
      "Epoch 5/100\n",
      "25838/25838 [==============================] - 193s 7ms/step - loss: 1.8279 - acc: 0.2506 - val_loss: 1.8178 - val_acc: 0.2489\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.82272 to 1.81781, saving model to D:/Github Projects/CNN/Facial Expression/weights.hd5\n",
      "Epoch 6/100\n",
      "25838/25838 [==============================] - 304s 12ms/step - loss: 1.8211 - acc: 0.2515 - val_loss: 1.8150 - val_acc: 0.2489\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.81781 to 1.81504, saving model to D:/Github Projects/CNN/Facial Expression/weights.hd5\n",
      "Epoch 7/100\n",
      "25838/25838 [==============================] - 242s 9ms/step - loss: 1.8191 - acc: 0.2519 - val_loss: 1.8164 - val_acc: 0.2489\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.81504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1696004c4e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3),input_shape=(48,48,1),activation='relu',kernel_regularizer=l2(0.01)))\n",
    "model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(512,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7,activation='softmax'))\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "adam=optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
    "early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
    "checkpointer = ModelCheckpoint('D:/Github Projects/CNN/Facial Expression/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(\n",
    "          train_data,\n",
    "          train_label,\n",
    "          epochs = epochs,\n",
    "          batch_size = batch_size,\n",
    "          validation_split = 0.2,\n",
    "          shuffle = True,\n",
    "          callbacks=[lr_reducer, checkpointer, early_stopper]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_label = np.argmax(model.predict(test_data), axis=1)\n",
    "test_label = np.argmax(test_label, axis=1)\n",
    "print (\"Accuracy score = \", accuracy_score(test_label, predicted_test_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
