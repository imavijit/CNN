{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial  Expression Identifier using CNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfjfwBpukDxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fn_QXzz2pHG",
        "colab_type": "text"
      },
      "source": [
        "The Dataset is taken form :https://www.kaggle.com/deadskull7/fer2013\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR1BUKKvpUD7",
        "colab_type": "code",
        "outputId": "ce1ae6f4-398d-44c4-87d1-cfd647a5716b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyI3HdPvnmqV",
        "colab_type": "code",
        "outputId": "39d5a1cf-f2c9-488e-b57a-c803193fa86c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Activation, Dropout, MaxPooling2D, Flatten, Dense\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import optimizers\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUAAXhbOnvhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path):\n",
        "    train_data=[]\n",
        "    train_label=[]\n",
        "    test_data=[]\n",
        "    test_label=[]\n",
        "    with open(data_path,'r') as file:\n",
        "        for line_no,line in enumerate(file.readlines()):\n",
        "            if(0<line_no<=35887):\n",
        "                class_type,pixels,use_type=line.split(',')\n",
        "                \n",
        "                #In dataest images are in 48*48 dimesion.So we extract the image data and rearrange it to 48*48 array\n",
        "                image_data=np.asarray([int(x) for x in pixels.split()]).reshape(48,48)  \n",
        "                \n",
        "                #We convert the image data into unsigned integer and divide it by 255 to normalize the data.\n",
        "                #255 is the maximum possible value of a single cell. By dividing every element by 255, we ensure that all our values range between 0 and 1.\n",
        "                image_data=image_data.astype(np.uint8)/255.0\n",
        "              \n",
        "                if (use_type.strip() == 'PrivateTest'):    \n",
        "                    test_data.append(image_data)\n",
        "                    test_label.append(class_type)\n",
        "                else:\n",
        "                    train_data.append(image_data)\n",
        "                    train_label.append(class_type)\n",
        "        \n",
        "        #we will expand the dimensions of both testing and training data by one to accommodate the channel \n",
        "        test_data=np.expand_dims(test_data,-1)\n",
        "        train_data=np.expand_dims(train_data,-1)\n",
        "        \n",
        "        #one hot encode all the labels using the to_categorical() function \n",
        "        test_label=to_categorical(test_label, num_classes=7)\n",
        "        train_label=to_categorical(train_label, num_classes=7)\n",
        "        \n",
        "        return np.array(train_data),np.array(train_label),np.array(test_data),np.array(test_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyX5M0eMoCg3",
        "colab_type": "code",
        "outputId": "0fde233b-90cc-4635-962e-388665580da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#load the data\n",
        "path=\"/content/gdrive/My Drive/Colab Notebooks/fer2013.csv\"\n",
        "train_data,train_label,test_data,test_label=load_data(path)\n",
        "print(\"Number of images in Training set:\", len(train_data))\n",
        "print(\"Number of images in Test set:\", len(test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Training set: 32298\n",
            "Number of images in Test set: 3589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohUuS3wTp-XP",
        "colab_type": "code",
        "outputId": "df22d73c-2857-4bfe-e641-714892dd2b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=Sequential()\n",
        "\n",
        "model.add(Conv2D(64,(3,3),input_shape=(48,48,1),activation='relu',kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(512,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512,(3,3),padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "adam=optimizers.Adam(lr=learning_rate)\n",
        "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "print(model.summary)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "#early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/gdrive/My Drive/Colab Notebooks/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(\n",
        "          train_data,\n",
        "          train_label,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.2,\n",
        "          shuffle = True,\n",
        "          callbacks=[lr_reducer,checkpointer]\n",
        "          )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0625 22:41:59.267386 139710620465024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0625 22:41:59.283449 139710620465024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0625 22:41:59.286869 139710620465024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0625 22:41:59.326290 139710620465024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0625 22:41:59.327161 139710620465024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0625 22:41:59.939531 139710620465024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0625 22:42:00.005438 139710620465024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0625 22:42:00.013786 139710620465024 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0625 22:42:01.051295 139710620465024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0625 22:42:01.166471 139710620465024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x7f1098794748>>\n",
            "Train on 25838 samples, validate on 6460 samples\n",
            "Epoch 1/100\n",
            "25838/25838 [==============================] - 38s 1ms/step - loss: 2.0823 - acc: 0.2052 - val_loss: 1.8720 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.87198, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 2/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8663 - acc: 0.2414 - val_loss: 1.8432 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.87198 to 1.84319, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 3/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8428 - acc: 0.2482 - val_loss: 1.8261 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.84319 to 1.82614, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 4/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8340 - acc: 0.2501 - val_loss: 1.8249 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.82614 to 1.82486, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 5/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8277 - acc: 0.2513 - val_loss: 1.8208 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.82486 to 1.82084, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 6/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.8220 - acc: 0.2517 - val_loss: 1.8152 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.82084 to 1.81525, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 7/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.8186 - acc: 0.2518 - val_loss: 1.8130 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.81525 to 1.81298, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 8/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.8165 - acc: 0.2513 - val_loss: 1.8055 - val_acc: 0.2491\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.81298 to 1.80549, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 9/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.8123 - acc: 0.2503 - val_loss: 1.7984 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.80549 to 1.79842, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 10/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8105 - acc: 0.2511 - val_loss: 1.8087 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.79842\n",
            "Epoch 11/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8088 - acc: 0.2517 - val_loss: 1.8319 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.79842\n",
            "Epoch 12/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8056 - acc: 0.2523 - val_loss: 1.7971 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.79842 to 1.79713, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 13/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.8033 - acc: 0.2521 - val_loss: 1.8112 - val_acc: 0.2488\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.79713\n",
            "Epoch 14/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.8023 - acc: 0.2527 - val_loss: 1.8292 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.79713\n",
            "Epoch 15/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8047 - acc: 0.2510 - val_loss: 1.8141 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.79713\n",
            "Epoch 16/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.7994 - acc: 0.2535 - val_loss: 1.7967 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.79713 to 1.79667, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 17/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.7957 - acc: 0.2527 - val_loss: 1.8195 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.79667\n",
            "Epoch 18/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.7860 - acc: 0.2603 - val_loss: 1.7799 - val_acc: 0.2492\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.79667 to 1.77989, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 19/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.7674 - acc: 0.2719 - val_loss: 1.8635 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.77989\n",
            "Epoch 20/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.7366 - acc: 0.2897 - val_loss: 1.7056 - val_acc: 0.2960\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.77989 to 1.70561, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 21/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.6886 - acc: 0.3057 - val_loss: 1.7137 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.70561\n",
            "Epoch 22/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.6366 - acc: 0.3220 - val_loss: 1.6006 - val_acc: 0.3212\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.70561 to 1.60059, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 23/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.6020 - acc: 0.3462 - val_loss: 1.6057 - val_acc: 0.3615\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.60059\n",
            "Epoch 24/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.5812 - acc: 0.3637 - val_loss: 1.5255 - val_acc: 0.4053\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.60059 to 1.52551, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 25/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.5558 - acc: 0.3765 - val_loss: 1.5110 - val_acc: 0.4015\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.52551 to 1.51098, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 26/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.5308 - acc: 0.3883 - val_loss: 1.4645 - val_acc: 0.4091\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.51098 to 1.46447, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 27/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.5182 - acc: 0.3922 - val_loss: 1.4456 - val_acc: 0.4260\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.46447 to 1.44558, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 28/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.5013 - acc: 0.4011 - val_loss: 1.4554 - val_acc: 0.4008\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.44558\n",
            "Epoch 29/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4791 - acc: 0.4066 - val_loss: 1.3805 - val_acc: 0.4413\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.44558 to 1.38052, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 30/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4653 - acc: 0.4105 - val_loss: 1.6571 - val_acc: 0.2944\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.38052\n",
            "Epoch 31/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4498 - acc: 0.4195 - val_loss: 1.3989 - val_acc: 0.4297\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.38052\n",
            "Epoch 32/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4401 - acc: 0.4197 - val_loss: 1.3898 - val_acc: 0.4365\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.38052\n",
            "Epoch 33/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4328 - acc: 0.4214 - val_loss: 1.3911 - val_acc: 0.4276\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.38052\n",
            "Epoch 34/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4157 - acc: 0.4262 - val_loss: 1.3960 - val_acc: 0.4139\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.38052\n",
            "Epoch 35/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3967 - acc: 0.4369 - val_loss: 1.3804 - val_acc: 0.4257\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.38052 to 1.38038, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 36/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3855 - acc: 0.4363 - val_loss: 1.3388 - val_acc: 0.4495\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.38038 to 1.33880, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 37/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3776 - acc: 0.4423 - val_loss: 1.3349 - val_acc: 0.4502\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.33880 to 1.33487, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 38/100\n",
            "25838/25838 [==============================] - 33s 1ms/step - loss: 1.3629 - acc: 0.4469 - val_loss: 1.3656 - val_acc: 0.4367\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.33487\n",
            "Epoch 39/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3545 - acc: 0.4460 - val_loss: 1.3891 - val_acc: 0.4274\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.33487\n",
            "Epoch 40/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3467 - acc: 0.4518 - val_loss: 1.4214 - val_acc: 0.4102\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.33487\n",
            "Epoch 41/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3411 - acc: 0.4556 - val_loss: 1.3475 - val_acc: 0.4395\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.33487\n",
            "Epoch 42/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3252 - acc: 0.4606 - val_loss: 1.2885 - val_acc: 0.4690\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.33487 to 1.28853, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 43/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3180 - acc: 0.4643 - val_loss: 1.3804 - val_acc: 0.4240\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.28853\n",
            "Epoch 44/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3064 - acc: 0.4687 - val_loss: 1.3441 - val_acc: 0.4372\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.28853\n",
            "Epoch 45/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2956 - acc: 0.4674 - val_loss: 1.3432 - val_acc: 0.4593\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.28853\n",
            "Epoch 46/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2799 - acc: 0.4781 - val_loss: 1.3365 - val_acc: 0.4543\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.28853\n",
            "Epoch 47/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2696 - acc: 0.4796 - val_loss: 1.3402 - val_acc: 0.4478\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.28853\n",
            "Epoch 48/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2574 - acc: 0.4848 - val_loss: 1.3216 - val_acc: 0.4695\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.28853\n",
            "Epoch 49/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2526 - acc: 0.4908 - val_loss: 1.3473 - val_acc: 0.4594\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.28853\n",
            "Epoch 50/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2388 - acc: 0.4976 - val_loss: 1.2852 - val_acc: 0.4879\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.28853 to 1.28519, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 51/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2373 - acc: 0.4989 - val_loss: 1.2836 - val_acc: 0.4861\n",
            "\n",
            "Epoch 00051: val_loss improved from 1.28519 to 1.28356, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 52/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2250 - acc: 0.5043 - val_loss: 1.2749 - val_acc: 0.5009\n",
            "\n",
            "Epoch 00052: val_loss improved from 1.28356 to 1.27487, saving model to /content/gdrive/My Drive/Colab Notebooks/weights.hd5\n",
            "Epoch 53/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2140 - acc: 0.5093 - val_loss: 1.3208 - val_acc: 0.4726\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.27487\n",
            "Epoch 54/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2069 - acc: 0.5122 - val_loss: 1.2796 - val_acc: 0.5028\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.27487\n",
            "Epoch 55/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1994 - acc: 0.5207 - val_loss: 1.3161 - val_acc: 0.4817\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.27487\n",
            "Epoch 56/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1926 - acc: 0.5164 - val_loss: 1.2889 - val_acc: 0.4958\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.27487\n",
            "Epoch 57/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1730 - acc: 0.5266 - val_loss: 1.3119 - val_acc: 0.5090\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.27487\n",
            "Epoch 58/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1670 - acc: 0.5358 - val_loss: 1.2852 - val_acc: 0.5139\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.27487\n",
            "Epoch 59/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1541 - acc: 0.5401 - val_loss: 1.3159 - val_acc: 0.4830\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.27487\n",
            "Epoch 60/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1481 - acc: 0.5452 - val_loss: 1.3244 - val_acc: 0.4865\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.27487\n",
            "Epoch 61/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1387 - acc: 0.5507 - val_loss: 1.3032 - val_acc: 0.5076\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.27487\n",
            "Epoch 62/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1306 - acc: 0.5533 - val_loss: 1.2915 - val_acc: 0.5155\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.27487\n",
            "Epoch 63/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1160 - acc: 0.5573 - val_loss: 1.2799 - val_acc: 0.5149\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.27487\n",
            "Epoch 64/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1095 - acc: 0.5659 - val_loss: 1.3111 - val_acc: 0.5238\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.27487\n",
            "Epoch 65/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1023 - acc: 0.5652 - val_loss: 1.3010 - val_acc: 0.4991\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.27487\n",
            "Epoch 66/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0937 - acc: 0.5670 - val_loss: 1.3327 - val_acc: 0.5029\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.27487\n",
            "Epoch 67/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0809 - acc: 0.5767 - val_loss: 1.3010 - val_acc: 0.5241\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.27487\n",
            "Epoch 68/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0828 - acc: 0.5762 - val_loss: 1.3134 - val_acc: 0.5118\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.27487\n",
            "Epoch 69/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0673 - acc: 0.5820 - val_loss: 1.2938 - val_acc: 0.5161\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.27487\n",
            "Epoch 70/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0578 - acc: 0.5878 - val_loss: 1.2888 - val_acc: 0.5339\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.27487\n",
            "Epoch 71/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0512 - acc: 0.5872 - val_loss: 1.2994 - val_acc: 0.5282\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.27487\n",
            "Epoch 72/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0461 - acc: 0.5866 - val_loss: 1.3348 - val_acc: 0.5118\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.27487\n",
            "Epoch 73/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0421 - acc: 0.5920 - val_loss: 1.3260 - val_acc: 0.5204\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.27487\n",
            "Epoch 74/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0317 - acc: 0.5927 - val_loss: 1.3107 - val_acc: 0.5381\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.27487\n",
            "Epoch 75/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0163 - acc: 0.5997 - val_loss: 1.3237 - val_acc: 0.5322\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.27487\n",
            "Epoch 76/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0243 - acc: 0.6005 - val_loss: 1.3370 - val_acc: 0.5153\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.27487\n",
            "Epoch 77/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0086 - acc: 0.6060 - val_loss: 1.3451 - val_acc: 0.5280\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.27487\n",
            "Epoch 78/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9967 - acc: 0.6123 - val_loss: 1.3311 - val_acc: 0.5294\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.27487\n",
            "Epoch 79/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9991 - acc: 0.6113 - val_loss: 1.3292 - val_acc: 0.5421\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.27487\n",
            "Epoch 80/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9898 - acc: 0.6133 - val_loss: 1.3211 - val_acc: 0.5433\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.27487\n",
            "Epoch 81/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9762 - acc: 0.6166 - val_loss: 1.3097 - val_acc: 0.5407\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.27487\n",
            "Epoch 82/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9752 - acc: 0.6176 - val_loss: 1.3183 - val_acc: 0.5272\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.27487\n",
            "Epoch 83/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9682 - acc: 0.6213 - val_loss: 1.3335 - val_acc: 0.5438\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.27487\n",
            "Epoch 84/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9677 - acc: 0.6224 - val_loss: 1.3619 - val_acc: 0.5421\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.27487\n",
            "Epoch 85/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9614 - acc: 0.6289 - val_loss: 1.3602 - val_acc: 0.5433\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.27487\n",
            "Epoch 86/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9542 - acc: 0.6298 - val_loss: 1.3754 - val_acc: 0.5548\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.27487\n",
            "Epoch 87/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9481 - acc: 0.6298 - val_loss: 1.3505 - val_acc: 0.5433\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.27487\n",
            "Epoch 88/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9406 - acc: 0.6334 - val_loss: 1.3586 - val_acc: 0.5364\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.27487\n",
            "Epoch 89/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9390 - acc: 0.6349 - val_loss: 1.3565 - val_acc: 0.5424\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.27487\n",
            "Epoch 90/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9339 - acc: 0.6393 - val_loss: 1.3513 - val_acc: 0.5433\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.27487\n",
            "Epoch 91/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9281 - acc: 0.6410 - val_loss: 1.3485 - val_acc: 0.5525\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.27487\n",
            "Epoch 92/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9284 - acc: 0.6401 - val_loss: 1.3624 - val_acc: 0.5573\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.27487\n",
            "Epoch 93/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9184 - acc: 0.6430 - val_loss: 1.3989 - val_acc: 0.5485\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.27487\n",
            "Epoch 94/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9090 - acc: 0.6471 - val_loss: 1.3743 - val_acc: 0.5579\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.27487\n",
            "Epoch 95/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9115 - acc: 0.6507 - val_loss: 1.3806 - val_acc: 0.5430\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.27487\n",
            "Epoch 96/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9007 - acc: 0.6501 - val_loss: 1.4008 - val_acc: 0.5509\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.27487\n",
            "Epoch 97/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9002 - acc: 0.6497 - val_loss: 1.4158 - val_acc: 0.5633\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.27487\n",
            "Epoch 98/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9036 - acc: 0.6495 - val_loss: 1.4072 - val_acc: 0.5570\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.27487\n",
            "Epoch 99/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.8915 - acc: 0.6532 - val_loss: 1.4263 - val_acc: 0.5509\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.27487\n",
            "Epoch 100/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.8942 - acc: 0.6545 - val_loss: 1.4597 - val_acc: 0.5636\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.27487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f10804db780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2LlGqjYzK1m",
        "colab_type": "code",
        "outputId": "8b60b13f-eb4a-4c15-fbed-5bd4493734ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted_test_label = np.argmax(model.predict(test_data), axis=1)\n",
        "test_label = np.argmax(test_label, axis=1)\n",
        "print (\"Accuracy score = \", accuracy_score(test_label, predicted_test_label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score =  0.5611590972415714\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}